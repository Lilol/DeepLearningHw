@Misc{Trans2022,
  author       = {Stephen Ornes},
  note         = {Accessed: 09/11/2022},
  title        = {Will Transformers Take Over Artificial Intelligence?},
  year         = {2022},
  language     = {English},
  organization = {Quanta Magazine},
  url          = {https://www.quantamagazine.org/will-transformers-take-over-artificial-intelligence-20220310/},
}

@inproceedings{Cuenat2022,
  author    = {Cuenat, Stéphane and Couturier, Raphaël},
  booktitle = {2022 2nd International Conference on Computer, Control and Robotics (ICCCR)},
  title     = {Convolutional Neural Network (CNN) vs Vision Transformer (ViT) for Digital Holography},
  year      = {2022},
  pages     = {235-240},
  doi       = {10.1109/ICCCR54399.2022.9790134},
}

@Misc{TrvsCnn22021,
  author       = {Arjun Sarkar},
  month        = may,
  note         = {Accessed: 09/11/2022},
  title        = {Are Transformers better than CNN’s at Image Recognition?},
  year         = {2021},
  language     = {English},
  organization = {Towards Data Science},
  url          = {https://towardsdatascience.com/are-transformers-better-than-cnns-at-image-recognition-ced60ccc7c8},
}

@Misc{TrvsCnn2021,
  author       = {Pranoy Radhakrishnan},
  month        = aug,
  note         = {Accessed: 09/11/2022},
  title        = {Why Transformers are Slowly Replacing CNNs in Computer Vision?},
  year         = {2021},
  comment      = {CNN does not encode the relative position of different features. 

Large receptive fields are required in order to track long-range dependencies within an image.

a self attention module replaces the convolutional layer, so that now the model gets the ability to interact with pixels far away from its location.

More recently, researchers ran a series of experiments replacing some or all convolutional layers in ResNets with attention, and found the best performing models used convolutions in early layers and attention in later layers.},
  language     = {English},
  organization = {Becoming Human},
  url          = {https://becominghuman.ai/transformers-in-vision-e2e87b739feb},
}

@Misc{ViTHighPerformance,
  month        = may,
  note         = {Accessed: 09/11/2022},
  title        = {Why Are Vision Transformers So High Performance?},
  year         = {2022},
  language     = {English},
  organization = {AI-Scholar},
  url          = {https://ai-scholar.tech/en/articles/transformer/transformer-vs-cnn},
}

@inproceedings{Lu2021,
  author    = {Lu, Kangrui and Xu, Yuanrun and Yang, Yige},
  booktitle = {ICMLCA 2021; 2nd International Conference on Machine Learning and Computer Application},
  title     = {Comparison of the potential between transformer and CNN in image classification},
  year      = {2021},
  pages     = {1-6},
}

@Misc{TransvsCnn32021,
  author       = {Ram Sagar},
  month        = aug,
  note         = {Accessed: 09/11/2022},
  title        = {Are Visual Transformers Better Than CNNs},
  year         = {2021},
  language     = {English},
  organization = {Analitics India},
  url          = {https://analyticsindiamag.com/are-visual-transformers-better-than-convolutional-neural-networks/},
}

@inproceedings{Vaswani2017,
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Attention is All you Need},
  year      = {2017},
  editor    = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  publisher = {Curran Associates, Inc.},
  volume    = {30},
  url       = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
}

@Misc{Raghu2021,
  author    = {Raghu, Maithra and Unterthiner, Thomas and Kornblith, Simon and Zhang, Chiyuan and Dosovitskiy, Alexey},
  title     = {Do Vision Transformers See Like Convolutional Neural Networks?},
  year      = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.2108.08810},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences},
  publisher = {arXiv},
  url       = {https://arxiv.org/abs/2108.08810},
}

@Article{Khan2022,
  author    = {Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
  journal   = {ACM computing surveys (CSUR)},
  title     = {Transformers in vision: A survey},
  year      = {2022},
  note      = {Accessed: 09/11/2022},
  number    = {10s},
  pages     = {1--41},
  volume    = {54},
  publisher = {ACM New York, NY},
}

@inproceedings{Deng2009,
  author       = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle    = {2009 IEEE conference on computer vision and pattern recognition},
  title        = {Imagenet: A large-scale hierarchical image database},
  year         = {2009},
  organization = {Ieee},
  pages        = {248--255},
}

@Article{Steiner2021,
  author    = {Steiner, Andreas and Kolesnikov, Alexander and Zhai, Xiaohua and Wightman, Ross and Uszkoreit, Jakob and Beyer, Lucas},
  title     = {How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers},
  year      = {2021},
  note      = {Accessed: 09/11/2022},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.2106.10270},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences},
  publisher = {arXiv},
  url       = {https://arxiv.org/abs/2106.10270},
}

@Misc{Sayak2022,
  author       = {Sayak Paul},
  note         = {Accessed: 09/11/2022},
  title        = {Collection of new Vision Transformer models fine-tuned on ImageNet-1k.},
  keywords     = {TensorFlow Hub},
  language     = {English},
  organization = {TensorFlow},
  url          = {https://tfhub.dev/sayakpaul/collections/vision_transformer/1},
}

@Misc{Swimcat2020,
  author       = {Truong Hoang, Vinh},
  note         = {Accessed: 10/12/2022},
  title        = {Swimcat-ext},
  year         = {2020},
  doi          = {10.17632/vwdd9grvdp.1},
  organization = {Mendeley Data, V1},
  url          = {https://data.mendeley.com/datasets/vwdd9grvdp/1},
}

@inproceedings{Krizhevsky2012,
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1},
  title     = {ImageNet Classification with Deep Convolutional Neural Networks},
  year      = {2012},
  address   = {Red Hook, NY, USA},
  pages     = {1097–1105},
  publisher = {Curran Associates Inc.},
  series    = {NIPS'12},
  abstract  = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overriding in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  location  = {Lake Tahoe, Nevada},
  numpages  = {9},
}

@article{LeCun1989,
  author  = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
  journal = {Neural Computation},
  title   = {Backpropagation Applied to Handwritten Zip Code Recognition},
  year    = {1989},
  number  = {4},
  pages   = {541-551},
  volume  = {1},
  doi     = {10.1162/neco.1989.1.4.541},
}

@inproceedings{Zeiler2014,
  author    = {Zeiler, Matthew D. and Fergus, Rob},
  booktitle = {Computer Vision -- ECCV 2014},
  title     = {Visualizing and Understanding Convolutional Networks},
  year      = {2014},
  address   = {Cham},
  editor    = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  pages     = {818--833},
  publisher = {Springer International Publishing},
  abstract  = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
  isbn      = {978-3-319-10590-1},
}

@inproceedings{Karimi2021,
  author    = {Karimi, Davood and Vasylechko, Serge Didenko and Gholipour, Ali},
  booktitle = {Medical Image Computing and Computer Assisted Intervention -- MICCAI 2021},
  title     = {Convolution-Free Medical Image Segmentation Using Transformers},
  year      = {2021},
  address   = {Cham},
  editor    = {de Bruijne, Marleen and Cattin, Philippe C. and Cotin, St{\'e}phane and Padoy, Nicolas and Speidel, Stefanie and Zheng, Yefeng and Essert, Caroline},
  pages     = {78--88},
  publisher = {Springer International Publishing},
  abstract  = {Like other applications in computer vision, medical image segmentation and his email address have been most successfully addressed using deep learning models that rely on the convolution operation as their main building block. Convolutions enjoy important properties such as sparse interactions, weight sharing, and translation equivariance. These properties give convolutional neural networks (CNNs) a strong and useful inductive bias for vision tasks. However, the convolution operation also has important shortcomings: it performs a fixed operation on every test image regardless of the content and it cannot efficiently model long-range interactions. In this work we show that a network based on self-attention between neighboring patches and without any convolution operations can achieve better results. Given a 3D image block, our network divides it into {\$}{\$}n^3{\$}{\$}n33D patches, where {\$}{\$}n=3 {\backslash}text {\{} or {\}} 5{\$}{\$}n=3or5and computes a 1D embedding for each patch. The network predicts the segmentation map for the center patch of the block based on the self-attention between these patch embeddings. We show that the proposed model can achieve higher segmentation accuracies than a state of the art CNN. For scenarios with very few labeled images, we propose methods for pre-training the network on large corpora of unlabeled images. Our experiments show that with pre-training the advantage of our proposed network over CNNs can be significant when labeled training data is small.},
  isbn      = {978-3-030-87193-2},
}

@Article{Bazi2021,
  author         = {Bazi, Yakoub and Bashmal, Laila and Rahhal, Mohamad M. Al and Dayil, Reham Al and Ajlan, Naif Al},
  journal        = {Remote Sensing},
  title          = {Vision Transformers for Remote Sensing Image Classification},
  year           = {2021},
  issn           = {2072-4292},
  note           = {Accessed: 09/11/2022},
  number         = {3},
  volume         = {13},
  abstract       = {In this paper, we propose a remote-sensing scene-classification method based on vision transformers. These types of networks, which are now recognized as state-of-the-art models in natural language processing, do not rely on convolution layers as in standard convolutional neural networks (CNNs). Instead, they use multihead attention mechanisms as the main building block to derive long-range contextual relation between pixels in images. In a first step, the images under analysis are divided into patches, then converted to sequence by flattening and embedding. To keep information about the position, embedding position is added to these patches. Then, the resulting sequence is fed to several multihead attention layers for generating the final representation. At the classification stage, the first token sequence is fed to a softmax classification layer. To boost the classification performance, we explore several data augmentation strategies to generate additional data for training. Moreover, we show experimentally that we can compress the network by pruning half of the layers while keeping competing classification accuracies. Experimental results conducted on different remote-sensing image datasets demonstrate the promising capability of the model compared to state-of-the-art methods. Specifically, Vision Transformer obtains an average classification accuracy of 98.49%, 95.86%, 95.56% and 93.83% on Merced, AID, Optimal31 and NWPU datasets, respectively. While the compressed version obtained by removing half of the multihead attention layers yields 97.90%, 94.27%, 95.30% and 93.05%, respectively.},
  article-number = {516},
  doi            = {10.3390/rs13030516},
  url            = {https://www.mdpi.com/2072-4292/13/3/516},
}

@Misc{Tan2019,
  author = {Tan, Mingxing and Le, Quoc},
  month  = {05},
  note   = {Accessed: 09/11/2022},
  title  = {EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks},
  year   = {2019},
}

@inproceedings{He2016,
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Deep Residual Learning for Image Recognition},
  year      = {2016},
  pages     = {770-778},
  doi       = {10.1109/CVPR.2016.90},
}

@Misc{Li2022,
  author    = {Li, Yanghao and Mao, Hanzi and Girshick, Ross and He, Kaiming},
  note      = {Accessed: 09/11/2022},
  title     = {Exploring Plain Vision Transformer Backbones for Object Detection},
  year      = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license},
  doi       = {10.48550/ARXIV.2203.16527},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  publisher = {arXiv},
  url       = {https://arxiv.org/abs/2203.16527},
}

@Misc{Tfimm,
  author = {Bruveris, Martins},
  note   = {Accessed: 09/11/2022},
  title  = {Tfimm Library Documentation},
  year   = {2022},
}

@InProceedings{Dosovitskiy2021,
  author    = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
  booktitle = {9th International Conference on Learning Representations, {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021},
  title     = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  year      = {2021},
  publisher = {OpenReview.net},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/iclr/DosovitskiyB0WZ21.bib},
  timestamp = {Wed, 23 Jun 2021 17:36:39 +0200},
  url       = {https://openreview.net/forum?id=YicbFdNTTy},
}

@Misc{Keras2022,
  note  = {URL: https://keras.io/api/layers/preprocessing_layers/image_augmentation/, Accessed: 10/12/2022},
  title = {Keras Image Augmentation Layers},
  year  = {2022},
}

@InBook{Vilalta2011,
  author = {Vilalta, Ricardo and Giraud-Carrier, Christophe and Brazdil, Pavel and Soares, Carlos},
  pages  = {545-548},
  title  = {Inductive Transfer},
  year   = {2011},
  month  = {01},
  doi    = {10.1007/978-0-387-30164-8_401},
}

@Comment{jabref-meta: databaseType:bibtex;}
